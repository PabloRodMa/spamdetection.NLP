{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Sistema de detección de enlaces spam  \n",
                "\n",
                "\n",
                "Queremos implementar un sistema que sea capaz de detectar automáticamente si una página web contiene spam o no basándonos en su URL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
                        "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
                        "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
                        "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n"
                    ]
                }
            ],
            "source": [
                "# librerias \n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import nltk\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report\n",
                "import joblib\n",
                "# Descargar recursos de NLTK\n",
                "nltk.download('stopwords')\n",
                "nltk.download('punkt')\n",
                "nltk.download('wordnet')\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Paso 1: Cargar datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Datos cargados:\n",
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 2999 entries, 0 to 2998\n",
                        "Data columns (total 2 columns):\n",
                        " #   Column   Non-Null Count  Dtype \n",
                        "---  ------   --------------  ----- \n",
                        " 0   url      2999 non-null   object\n",
                        " 1   is_spam  2999 non-null   bool  \n",
                        "dtypes: bool(1), object(1)\n",
                        "memory usage: 26.5+ KB\n",
                        "None\n"
                    ]
                }
            ],
            "source": [
                "# Your code here\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "df = pd.read_csv(url)\n",
                "\n",
                "print(\"Datos cargados:\")\n",
                "print(df.head())\n",
                "print(df.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Paso 2: Preprocesamiento de URLs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "lemmatizer = WordNetLemmatizer()\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "def preprocess_url(url):\n",
                "    # Minúsculas\n",
                "    url = url.lower()\n",
                "    # Dividir por caracteres no alfanuméricos\n",
                "    tokens = re.split(r'\\W+', url)\n",
                "    # Filtrar stopwords y tokens vacíos\n",
                "    tokens = [t for t in tokens if t and t not in stop_words]\n",
                "    # Lemmatizar\n",
                "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
                "    return ' '.join(tokens)\n",
                "\n",
                "df['clean_url'] = df['url'].apply(preprocess_url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Index(['url', 'is_spam', 'clean_url'], dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "print(df.columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Dividir datos\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df['clean_url'], df['is_spam'], test_size=0.2, random_state=42, stratify=df['is_spam']\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Paso 3: Creamos modelo SVM basico"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resultados del modelo SVM inicial:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.96      0.96      0.96       461\n",
                        "        True       0.86      0.88      0.87       139\n",
                        "\n",
                        "    accuracy                           0.94       600\n",
                        "   macro avg       0.91      0.92      0.91       600\n",
                        "weighted avg       0.94      0.94      0.94       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "pipeline = Pipeline([\n",
                "    ('tfidf', TfidfVectorizer()),\n",
                "    ('svm', SVC())\n",
                "])\n",
                "\n",
                "pipeline.fit(X_train, y_train)\n",
                "y_pred = pipeline.predict(X_test)\n",
                "\n",
                "print(\"Resultados del modelo SVM inicial:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Paso 4: Optimizacion con GridSearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.3s\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.3s\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.3s\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.5s\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.4s\n",
                        "[CV] END svm__C=0.1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.5s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.6s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.3s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.2s\n",
                        "[CV] END svm__C=0.1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.2s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.6s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.5s\n",
                        "[CV] END svm__C=1, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.5s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.6s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.6s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.2s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.2s\n",
                        "[CV] END svm__C=1, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 1); total time=   0.2s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.6s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.6s\n",
                        "[CV] END svm__C=10, svm__kernel=linear, tfidf__ngram_range=(1, 2); total time=   0.5s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 1); total time=   0.4s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.6s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.7s\n",
                        "[CV] END svm__C=10, svm__kernel=rbf, tfidf__ngram_range=(1, 2); total time=   0.4s\n",
                        "Mejores parámetros: {'svm__C': 1, 'svm__kernel': 'rbf', 'tfidf__ngram_range': (1, 1)}\n",
                        "Resultados tras optimización:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "       False       0.96      0.96      0.96       461\n",
                        "        True       0.86      0.88      0.87       139\n",
                        "\n",
                        "    accuracy                           0.94       600\n",
                        "   macro avg       0.91      0.92      0.91       600\n",
                        "weighted avg       0.94      0.94      0.94       600\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "param_grid = {\n",
                "    'tfidf__ngram_range': [(1,1),(1,2)],\n",
                "    'svm__C': [0.1, 1, 10],\n",
                "    'svm__kernel': ['linear', 'rbf']\n",
                "}\n",
                "\n",
                "grid = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
                "grid.fit(X_train, y_train)\n",
                "\n",
                "print(\"Mejores parámetros:\", grid.best_params_)\n",
                "\n",
                "y_pred_best = grid.predict(X_test)\n",
                "print(\"Resultados tras optimización:\")\n",
                "print(classification_report(y_test, y_pred_best))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Paso 5: Guardamos el modelo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modelo guardado en models/url_spam_svm.pkl\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "# Crear carpeta si no existe\n",
                "os.makedirs(\"models\", exist_ok=True)\n",
                "\n",
                "# Guardar el modelo\n",
                "joblib.dump(grid.best_estimator_, \"models/url_spam_svm.pkl\")\n",
                "print(\"Modelo guardado en models/url_spam_svm.pkl\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
